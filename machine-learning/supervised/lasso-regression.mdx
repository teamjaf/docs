---
title: "Lasso Regression"
description: "Linear regression with $L_1$ regularization."
---

# ðŸŽ¯ Lasso Regression

<img src="../../images/supervised/lasso-regression.svg" alt="Lasso regression icon" width="80" />

Lasso regression uses an $L_1$ penalty to drive some coefficients exactly to zero, enabling feature selection.

## Hypothesis

The prediction is linear in the features:

$$
\hat{y} = \mathbf{x}^T \boldsymbol{\theta}.
$$

## Loss Function

The objective adds an $L_1$ term:

$$
J(\boldsymbol{\theta}) = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2 + \lambda \lVert \boldsymbol{\theta} \rVert_1.
$$

## Example (scikit-learn)

```python
import numpy as np
from sklearn.linear_model import Lasso

X = np.array([[1], [2], [3], [4]])
y = np.array([2.0, 3.0, 3.0, 5.0])

model = Lasso(alpha=0.1)
model.fit(X, y)

pred = model.predict([[5]])
print(pred)
```

## Use Case Example

Selecting relevant features when predicting energy usage from many sensor readings.

## Recommendations

- Standardize features to put them on the same scale.
- Use cross-validation to choose $\alpha$.
- Refer to the [scikit-learn guide](https://scikit-learn.org/stable/modules/linear_model.html#lasso) for additional tips.
