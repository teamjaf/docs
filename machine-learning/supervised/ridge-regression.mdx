---
title: "Ridge Regression"
description: "Linear regression with $L_2$ regularization."
---

# üèîÔ∏è Ridge Regression

<img src="../../images/supervised/ridge-regression.svg" alt="Ridge regression icon" width="80" />

Ridge regression, also called $L_2$-regularized regression, adds a penalty on the magnitude of coefficients to reduce overfitting.

## Hypothesis

For feature vector $\mathbf{x} = [1, x_1, \dots, x_n]^T$,

$$
\hat{y} = \mathbf{x}^T \boldsymbol{\theta}.
$$

## Loss Function

The cost adds an $L_2$ penalty:

$$
J(\boldsymbol{\theta}) = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2 + \lambda \lVert \boldsymbol{\theta} \rVert_2^2.
$$

## Example (scikit-learn)

```python
import numpy as np
from sklearn.linear_model import Ridge

X = np.array([[1], [2], [3], [4]])
y = np.array([2.0, 3.0, 3.0, 5.0])

model = Ridge(alpha=1.0)
model.fit(X, y)

pred = model.predict([[5]])
print(pred)
```

## Use Case Example

Predicting house prices with many correlated features.

## Recommendations

- Scale features before training for stable solutions.
- Tune $\alpha$ using cross-validation.
- See the [scikit-learn docs](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression) for more details.
